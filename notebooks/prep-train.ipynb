{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd00c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BloomModel, \n",
    "    AutoModel, \n",
    "    BloomForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14477df",
   "metadata": {},
   "source": [
    "# Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and prepping data...\")\n",
    "json1_path = \"../data/CMS_VWSC_â€“_WSC_Round_9_1.2_Hours_of_Sebring.xml\"\n",
    "with open(json1_path) as file:\n",
    "    input_data_1 = file.read()\n",
    "\n",
    "summary1_path = \"../data/CMS_VWSC_-_WSC_Round_9_1.2_Hours_of_Sebring.txt\"\n",
    "with open(summary1_path, \"r\") as file:\n",
    "    summary_1 = file.read()\n",
    "    \n",
    "json2_path = \"../data/NARS_at_The_Green_Hell_2020.xml\"\n",
    "with open(json2_path) as file:\n",
    "    input_data_2 = file.read()\n",
    "\n",
    "summary2_path = \"../data/NARS_at_The_Green_Hell_2020.txt\"\n",
    "with open(summary2_path, \"r\") as file:\n",
    "    summary_2 = file.read()\n",
    "\n",
    "json3_path = \"../data/VWSC_2.4_Hours_of_Le_Mans_2020.xml\" \n",
    "with open(json3_path) as file:\n",
    "    input_data_3 = file.read()\n",
    "\n",
    "summary3_path = \"../data/VWSC_2.4_Hours_of_Le_Mans_2020.txt\"\n",
    "with open(summary3_path, \"r\") as file:\n",
    "    summary_3 = file.read()\n",
    "\n",
    "json4_path = \"../data/VWSC_60_Minutes_of_Laguna_Seca_2020.xml\"\n",
    "with open(json4_path) as file:\n",
    "    input_data_4 = file.read()\n",
    "\n",
    "summary4_path = \"../data/VWSC_60_Minutes_of_Laguna_Seca_2020.txt\"\n",
    "with open(summary4_path, \"r\") as file:\n",
    "    summary_4 = file.read()\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "df_train[\"input\"] = [input_data_1, input_data_2, input_data_3]\n",
    "df_train[\"output\"] = [summary_1, summary_2, summary_3]\n",
    "df_train.head()\n",
    "\n",
    "jsonl_filename = \"230315-cms.jsonl\"\n",
    "df_train.to_json(jsonl_filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c76b4",
   "metadata": {},
   "source": [
    "# Model training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up model training...\")\n",
    "SEED_VALUE = 42\n",
    "MODEL_NAME = \"bigscience/bloom-1b1\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "\n",
    "task_designator = \"Summary:\"\n",
    "context_length = 2048\n",
    "padding = \"max_length\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BloomForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir= f\"fine-tuned/bloom_1b1_summarizer_{EPOCHS}_epochs\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    logging_steps=1000,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=5e-6,\n",
    "    fp16=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why the validation is set to the same as the file?\n",
    "data = load_dataset(\"json\", data_files={\"train\":[jsonl_filename], \"validation\":[jsonl_filename]})\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c14bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    ip = \"\"\n",
    "    for ele in element[\"input\"]:\n",
    "        ip += str(ele)\n",
    "    print(ip)\n",
    "    text = \"Data: \" + ip + \"\\n\" + task_designator + \" \" + element[\"output\"] + tokenizer.eos_token\n",
    "    output = tokenizer(\n",
    "        text, \n",
    "        truncation=True,\n",
    "        padding=padding,\n",
    "        max_length=context_length,\n",
    "        )\n",
    "    \n",
    "    labels = output[\"input_ids\"].copy()\n",
    "    labels = [-100 if ele == tokenizer.pad_token_id else ele for ele in labels]\n",
    "    output[\"labels\"] = labels\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(\n",
    "    tokenize, remove_columns=data[\"train\"].column_names\n",
    ")\n",
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b76562",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce37d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a829ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = f\"fine-tuned/bloom_1b1_summarizer_{EPOCHS}_epochs/checkpoint-20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\", model=checkpoint, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d81bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(example):  \n",
    "    \n",
    "    prompt = \"Data: \" + example + \"\\n\" + task_designator \n",
    "\n",
    "    # currently generating 400 max tokens\n",
    "    outputs = generator(prompt, max_new_tokens=400)\n",
    "    output_str = outputs[0][\"generated_text\"]\n",
    "\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = \"../data/VWSC_80_Minutes_of_Bahrain___Season_End_Report.xml\"\n",
    "with open(ip, \"r\") as file:\n",
    "    results_to_generate = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(\"*******************************\")\n",
    "print(\"Generating summary...\")\n",
    "op = generate_summary(ip)\n",
    "pred_op = op.split(task_designator)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdf32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4baed51473791ef0232b8abb2e18d3f9d4082a2ae04d69dda84da8d3d3c6275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
